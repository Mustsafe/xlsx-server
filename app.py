from flask import Flask, request, jsonify, Response
import pandas as pd
import os
import re
import json
import difflib
import requests
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List
from urllib.parse import quote
from datetime import datetime, timedelta
from dateutil import parser
import openai
import logging

from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
from openpyxl.utils import get_column_letter

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
openai.api_key = os.getenv("OPENAI_API_KEY")
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)

# ìœ í‹¸ í•¨ìˆ˜

def sanitize(text: str) -> str:
    return re.sub(r"[^0-9a-zê°€-í£]", "", text.lower())

# í…œí”Œë¦¿ëª… alias ë§¤í•‘

def build_alias_map(templates: List[str]) -> dict:
    alias = {}
    for tpl in templates:
        key = sanitize(tpl)
        alias[key] = tpl
    return alias

# í‚¤ì›Œë“œë¡œ í…œí”Œë¦¿ resolve

def resolve_keyword(raw: str, templates: List[str], alias_map: dict, freq: dict) -> str:
    query = re.sub(r"\s*(?:ì–‘ì‹|ì„œì‹)(?:ì„|ë¥¼)?$", "", raw.strip(), flags=re.IGNORECASE)
    key = sanitize(query)
    if key in alias_map:
        return alias_map[key]
    norms = [sanitize(t) for t in templates]
    match = difflib.get_close_matches(key, norms, n=1, cutoff=0.6)
    if match:
        return templates[norms.index(match[0])]
    raise ValueError(f"í…œí”Œë¦¿ '{raw}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# JSON ë°°ì—´ í–‰ ë¶„í•´ (BOMÂ·ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ ì²˜ë¦¬ í¬í•¨)

def explode_json_rows(df: pd.DataFrame) -> pd.DataFrame:
    records = []
    for _, row in df.iterrows():
        raw_text = row.get("ì‘ì„± ì–‘ì‹", "")
        text = raw_text.lstrip("\ufeff").strip()
        text = text.replace("â€œ", '"').replace("â€", '"')
        try:
            arr = json.loads(text) if text.startswith("[") else None
        except json.JSONDecodeError:
            arr = None
        if isinstance(arr, list):
            for elem in arr:
                new = row.copy()
                new["ì‘ì—… í•­ëª©"] = elem.get("í•­ëª©", "")
                new["ì‘ì„± ì–‘ì‹"] = elem.get("ë‚´ìš©", elem.get("ì„¸ë¶€ì‚¬í•­", ""))
                records.append(new)
        else:
            records.append(row)
    return pd.DataFrame(records)

@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    try:
        raw = request.args.get("template", "")
        path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
        if not os.path.exists(path):
            return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404

        df = pd.read_csv(path, encoding="utf-8-sig")
        templates = df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist()
        alias_map = build_alias_map(templates)\n        freq = df["í…œí”Œë¦¿ëª…"].value_counts().to_dict()

        try:
            tpl = resolve_keyword(raw, templates, alias_map, freq)
            out_df = df[df["í…œí”Œë¦¿ëª…"] == tpl][["ì‘ì—… í•­ëª©","ì‘ì„± ì–‘ì‹","ì‹¤ë¬´ ì˜ˆì‹œ 1","ì‹¤ë¬´ ì˜ˆì‹œ 2"]].copy()
        except ValueError:
            tpl = raw
            system = {"role":"system","content":"ì‚°ì—…ì•ˆì „ ë¬¸ì„œ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON ë°°ì—´ì„ 5ê°œ ì´ìƒ ìƒì„±í•˜ì„¸ìš”."}
            user = {"role":"user","content":f"í…œí”Œë¦¿ëª… '{raw}'ì˜ ê¸°ë³¸ ì–‘ì‹ì„ JSON ë°°ì—´ë¡œ ì£¼ì„¸ìš”."}
            resp = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[system, user],
                max_tokens=800,
                temperature=0.7
            )
            try:
                data = json.loads(resp.choices[0].message.content)
                out_df = pd.DataFrame(data)
            except:
                out_df = pd.DataFrame([{
                    "ì‘ì—… í•­ëª©": raw,
                    "ì‘ì„± ì–‘ì‹": resp.choices[0].message.content.replace("\n"," "),
                    "ì‹¤ë¬´ ì˜ˆì‹œ 1": "",
                    "ì‹¤ë¬´ ì˜ˆì‹œ 2": ""
                }])

        # JSON ë°°ì—´ ë¶„í•´
        out_df = explode_json_rows(out_df)

        # AI ë™ì  ê³ ë„í™”
        for idx, row in out_df.iterrows():
            base = row["ì‘ì„± ì–‘ì‹"]
            if isinstance(base, str) and len(base.splitlines()) < 3:
                sys_msg = {"role":"system","content":"5~8ê°œ ì ê²€ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë°°ì—´ë¡œ ìƒì„±í•˜ì„¸ìš”."}
                usr_msg = {"role":"user","content":json.dumps({"base": base})}
                try:
                    r = openai.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[sys_msg, usr_msg],
                        max_tokens=300,
                        temperature=0.7
                    )
                    items = json.loads(r.choices[0].message.content)
                    if isinstance(items, list):
                        out_df.at[idx, "ì‘ì„± ì–‘ì‹"] = "\n".join(items)
                except:
                    pass
            for ex in ["ì‹¤ë¬´ ì˜ˆì‹œ 1","ì‹¤ë¬´ ì˜ˆì‹œ 2"]:
                ex_base = row.get(ex, "")
                if ex_base:
                    sysg = {"role":"system","content":"êµ¬ì²´ì  í˜„ì¥ ì‚¬ë¡€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."}
                    usrg = {"role":"user","content":json.dumps({"base": ex_base})}
                    try:
                        rr = openai.chat.completions.create(
                            model="gpt-4o-mini",
                            messages=[sysg, usrg],
                            max_tokens=100,
                            temperature=0.7
                        )
                        out_df.at[idx, ex] = rr.choices[0].message.content.strip()
                    except:
                        pass

        # ìˆœì„œ ì¬ì •ë ¬
        order = ["ğŸ“‹ ì‘ì—… ì ˆì°¨","ğŸ’¡ ì‹¤ë¬´ ê°€ì´ë“œ","âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸","ğŸ› ï¸ ìœ ì§€ë³´ìˆ˜ í¬ì¸íŠ¸","ğŸ“ ì¶œì²˜"]
        out_df["_order"] = out_df["ì‘ì—… í•­ëª©"].apply(lambda x: order.index(x) if x in order else 99)
        out_df = out_df.sort_values("_order").drop(columns=["_order"])

        # ì—‘ì…€ ìƒì„± & í¬ë§·
        wb = Workbook()
        ws = wb.active
        headers = ["ì‘ì—… í•­ëª©","ì‘ì„± ì–‘ì‹","ì‹¤ë¬´ ì˜ˆì‹œ 1","ì‹¤ë¬´ ì˜ˆì‹œ 2"]
        ws.append(headers)
        for cell in ws[1]:
            cell.font = Font(bold=True)
            cell.alignment = Alignment(horizontal="center", vertical="center")
        for row in out_df.itertuples(index=False):
            ws.append(row)
        for i, col in enumerate(ws.columns, 1):
            mx = max(len(str(c.value or "")) for c in col)
            ws.column_dimensions[get_column_letter(i)].width = min(mx+2,60)
            for c in col[1:]:
                c.alignment = Alignment(wrap_text=True, vertical="top", horizontal="left")

        buf = BytesIO()
        wb.save(buf)
        buf.seek(0)
        filename = f"{tpl}.xlsx" if tpl else f"{raw}.xlsx"
        disp = quote(filename)
        return Response(
            buf.read(),
            headers={
                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "Content-Disposition": f"attachment; filename*=UTF-8''{disp}",
                "Cache-Control": "public, max-age=3600"
            }
        )
    except Exception as e:
        logger.exception("create_xlsx error")
        return jsonify(error=f"ì„œë²„ ì˜¤ë¥˜: {e}"), 500

# ë‰´ìŠ¤ í¬ë¡¤ë§ & ë Œë”ë§ ë¡œì§

def fetch_safetynews_article_content(url):
    try:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else ""
    except:
        return ""

def crawl_naver_news():
    base = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id":NAVER_CLIENT_ID, "X-Naver-Client-Secret":NAVER_CLIENT_SECRET}
    kws = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out=[]
    for kw in kws:
        r = requests.get(base, headers=headers, params={"query":kw, "display":2, "sort":"date"}, timeout=10)
        if r.status_code==200:
            for item in r.json().get("items",[]):
                title = BeautifulSoup(item["title"],"html.parser").get_text()
                desc  = BeautifulSoup(item["description"],"html.parser").get_text()

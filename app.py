from flask import Flask, request, jsonify, send_from_directory, Response
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import openai
import difflib
from dateutil import parser
from datetime import datetime, timedelta
from io import BytesIO
from typing import List
from urllib.parse import quote
import json
import logging

# â”€â”€ ë¡œê±° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

# â”€â”€ Flask & í™˜ê²½ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
openai.api_key      = os.getenv("OPENAI_API_KEY")
NAVER_CLIENT_ID     = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# â”€â”€ ë°ì´í„° ë””ë ‰í† ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)

# â”€â”€ alias_map / resolve_keyword (app (1).py ë²„ì „ ê·¸ëŒ€ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_alias_map(template_list: List[str]) -> dict:
    alias = {}
    SUFFIXES = [" ì ê²€í‘œ", " ê³„íšì„œ", " ì„œì‹", " í‘œ", "ì–‘ì‹", " ì–‘ì‹", "_ì–‘ì‹"]
    for tpl in template_list:
        alias[tpl] = tpl
        alias[tpl.replace("_", " ")] = tpl
        alias[tpl.replace(" ", "_")] = tpl

        low = tpl.lower()
        alias[low] = tpl
        alias[low.replace("_", " ")] = tpl

        base_space = tpl.replace("_", " ")
        nospace = base_space.replace(" ", "").lower()
        alias[nospace] = tpl

        for suf in SUFFIXES:
            combo = base_space + suf
            alias[combo] = tpl
            alias[combo.replace(" ", "_")] = tpl
            alias[combo.lower()] = tpl

    for tpl in template_list:
        norm = tpl.lower().replace(" ", "").replace("_", "")
        if "jsa" in norm or "ì‘ì—…ì•ˆì „ë¶„ì„" in norm:
            alias["__FORCE_JSA__"] = tpl
        if "loto" in norm:
            alias["__FORCE_LOTO__"] = tpl

    temp = {}
    for k, v in alias.items():
        temp[k.replace(" ", "_")] = v
        temp[k.replace("_", " ")] = v
    alias.update(temp)
    return alias

def resolve_keyword(raw_keyword: str, template_list: List[str], alias_map: dict) -> str:
    raw = raw_keyword.strip()
    norm = raw.replace("_", " ").replace("-", " ")
    key_lower = norm.lower()
    cleaned_key = key_lower.replace(" ", "")

    # JSA/LOTO ìš°ì„  ë§¤í•‘
    if "__FORCE_JSA__" in alias_map and ("jsa" in cleaned_key or "ì‘ì—…ì•ˆì „ë¶„ì„" in cleaned_key):
        return alias_map["__FORCE_JSA__"]
    if "__FORCE_LOTO__" in alias_map and "loto" in cleaned_key:
        return alias_map["__FORCE_LOTO__"]

    # ì™„ì „ ì¼ì¹˜
    for tpl in template_list:
        tpl_norm = tpl.lower().replace(" ", "").replace("_", "")
        if key_lower == tpl.lower() or cleaned_key == tpl_norm:
            return tpl

    # í† í° ê¸°ë°˜ ë§¤ì¹­
    tokens = [t for t in key_lower.split(" ") if t]
    candidates = [tpl for tpl in template_list if all(tok in tpl.lower() for tok in tokens)]
    if len(candidates) == 1:
        return candidates[0]

    # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
    substr_cands = [
        tpl for tpl in template_list
        if cleaned_key in tpl.lower().replace(" ", "").replace("_", "")
    ]
    if len(substr_cands) == 1:
        return substr_cands[0]

    # alias_map ì§ì ‘ ì¡°íšŒ
    if raw in alias_map:
        return alias_map[raw]
    if key_lower in alias_map:
        return alias_map[key_lower]

    # í¼ì§€ ë§¤ì¹­
    candidates_norm = [t.replace(" ", "").replace("_", "").lower() for t in template_list]
    matches = difflib.get_close_matches(cleaned_key, candidates_norm, n=1, cutoff=0.6)
    if matches:
        return template_list[candidates_norm.index(matches[0])]

    raise ValueError(f"í…œí”Œë¦¿ '{raw_keyword}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•í•œ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° endpoints: /health, /list_templates, /create_xlsx, /daily_news, /render_news", 200

@app.route("/health", methods=["GET"])
def health_check():
    return "OK", 200

@app.route("/list_templates", methods=["GET"])
def list_templates():
    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    templates = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    return jsonify({
        "template_list": templates,
        "alias_keys": sorted(build_alias_map(templates).keys())
    })

# â”€â”€ ìˆ˜ì •ëœ create_xlsx: backup ì˜ ë§¤í•‘ ê·¸ëŒ€ë¡œ, 500 ì—ëŸ¬ ì—†ë„ë¡ ì™„ì „ ë°˜í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "").strip()
    # â€œì–‘ì‹/ì„œì‹/ì ê²€í‘œ/ê³„íšì„œ/í‘œâ€ ì–´ë¯¸ ì œê±°
    raw = re.sub(r"(ì–‘ì‹|ì„œì‹|ì ê²€í‘œ|ê³„íšì„œ|í‘œ)(ì„|ë¥¼)?\s*(ì£¼ì„¸ìš”|ì¤˜)?$", r"\1", raw, flags=re.IGNORECASE)

    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    if "í…œí”Œë¦¿ëª…" not in df.columns:
        return jsonify(error="í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."), 500

    templates = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    alias_map = build_alias_map(templates)

    try:
        # 1) ê³ ë„í™”ëœ 70ì¢… ì–‘ì‹ ë§¤ì¹­
        tpl = resolve_keyword(raw, templates, alias_map)
        logger.info(f"Matched template: {tpl}")
        out_df = df[df["í…œí”Œë¦¿ëª…"] == tpl][
            ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]
        ]
    except ValueError as e:
        # 2) ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì„ì‹œ ì–‘ì‹ ìƒì„±
        logger.warning(str(e))
        out_df = pd.DataFrame([{
            "ì‘ì—… í•­ëª©": raw,
            "ì‘ì„± ì–‘ì‹": "[ì—¬ê¸°ì— ì–‘ì‹ í•­ëª©ì„ ì…ë ¥í•˜ì„¸ìš”]",
            "ì‹¤ë¬´ ì˜ˆì‹œ 1": "",
            "ì‹¤ë¬´ ì˜ˆì‹œ 2": ""
        }])

    # 3) ì—‘ì…€ ìƒì„±
    wb = Workbook()
    ws = wb.active
    headers = ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]
    ws.append(headers)
    for cell in ws[1]:
        cell.font = Font(bold=True)
    for row in out_df.itertuples(index=False):
        ws.append(row)

    buf = BytesIO()
    wb.save(buf)
    buf.seek(0)

    filename = f"{tpl}.xlsx" if 'tpl' in locals() else f"{raw}.xlsx"
    disposition = "attachment; filename*=UTF-8''" + quote(filename)
    resp_headers = {
        "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "Content-Disposition": disposition,
        "Cache-Control": "public, max-age=3600"
    }
    return Response(buf.read(), headers=resp_headers)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ ë‰´ìŠ¤ í¬ë¡¤ë§ / ë Œë”ë§ ë¡œì§ (ì›ë³¸ ê·¸ëŒ€ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_safetynews_article_content(url):
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else ""
    except:
        return ""

def crawl_naver_news():
    base = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    kws = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in kws:
        r = requests.get(base, headers=headers, params={"query":kw,"display":2,"sort":"date"}, timeout=10)
        if r.status_code != 200: continue
        for item in r.json().get("items", []):
            title = BeautifulSoup(item["title"], "html.parser").get_text()
            desc  = BeautifulSoup(item["description"], "html.parser").get_text()
            out.append({"ì¶œì²˜": item.get("originallink","ë„¤ì´ë²„"), "ì œëª©": title,
                        "ë§í¬": item.get("link",""), "ë‚ ì§œ": item.get("pubDate",""), "ë³¸ë¬¸": desc})
    return out

def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    kws = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in kws:
        r = requests.get(f"{base}/search/news?searchword={kw}", headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code != 200: continue
        soup = BeautifulSoup(r.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else ""
            d = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({"ì¶œì²˜":"ì•ˆì „ì‹ ë¬¸","ì œëª©": t.get_text(strip=True) if t else "",
                        "ë§í¬": href, "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                        "ë³¸ë¬¸": content[:1000]})
    return out

@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200
    return jsonify(news)

@app.route("/render_news", methods=["GET"])
def render_news():
    news = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in news:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)
    items = sorted(filtered, key=lambda x: parser.parse(x["ë‚ ì§œ"]), reverse=True)[:3]
    if not items:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200

    template = ("ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
                "ğŸ“° â€œ{title}â€ ({date}, {ì¶œì²˜})\n\n"
                "{ë³¸ë¬¸}\n"
                "ğŸ” ë” ë³´ë ¤ë©´ â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    system_msg = {"role":"system","content":f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template}"}
    user_msg = {"role":"user","content":str(items)}
    resp = openai.chat.completions.create(
        model="gpt-4o-mini", messages=[system_msg, user_msg], max_tokens=800, temperature=0.7
    )
    return jsonify(formatted_news=resp.choices[0].message.content)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

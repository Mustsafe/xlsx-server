from flask import Flask, request, send_file, jsonify, send_from_directory
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import openai
import difflib
from dateutil import parser
from datetime import datetime, timedelta
from io import BytesIO
from typing import List

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # í•œê¸€ ê¹¨ì§ ë°©ì§€

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai.api_key = os.getenv("OPENAI_API_KEY")

# ./data ë””ë ‰í† ë¦¬ ì‚¬ìš©
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì„œë¹™
@app.route("/.well-known/<path:filename>")
def serve_well_known(filename):
    return send_from_directory(
        os.path.join(app.root_path, "static", ".well-known"),
        filename,
        mimetype="application/json"
    )

# OpenAPI ë° ë¡œê³  íŒŒì¼ ì„œë¹™
@app.route("/openapi.json")
def serve_openapi():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "openapi.json",
        mimetype="application/json"
    )

@app.route("/logo.png")
def serve_logo():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "logo.png",
        mimetype="image/png"
    )

# ë„¤ì´ë²„ ì˜¤í”ˆ API ìê²©ì¦ëª…
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# í‚¤ì›Œë“œ ë§¤í•‘ (êµ¬ì²´ í‚¤ ìš°ì„ , ê¸¸ì´ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë§¤ì¹­)
KEYWORD_ALIAS = {
    # ê¸°ì¡´ 11ì¢…
    "ê³ ì†Œì‘ì—… ì‚¬ì „ì ê²€í‘œ":         "ê³ ì†Œì‘ì—…_ì‚¬ì „ì ê²€í‘œ",
    "ê³ ì†Œì‘ì—… ê³„íšì„œ":            "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œ ì‘ì—… ê³„íšì„œ":           "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…":                  "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ":            "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„":                  "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ":            "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ":              "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ":           "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ":           "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ":          "ì–‘ì¤‘ê¸°_ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ":        "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ",

    # ì¶”ê°€ëœ 39ì¢…
    "ê°€ì‹œì„¤ì ê²€í‘œ":               "ê°€ì‹œì„¤ì ê²€í‘œ",
    "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ":         "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ",
    "êµëŒ€ê·¼ë¬´ê³„íší‘œ":             "êµëŒ€ê·¼ë¬´ê³„íší‘œ",
    "êµ¬ë‚´ë²„ìŠ¤ìš´í–‰ê´€ë¦¬ëŒ€ì¥":       "êµ¬ë‚´ë²„ìŠ¤ìš´í–‰ê´€ë¦¬ëŒ€ì¥",
    "êµ´ì‚­ê¸°ìš´ì „ê³„íšì„œ":           "êµ´ì‚­ê¸°ìš´ì „ê³„íšì„œ",
    "ìœ„í—˜ê¸°ê³„ì ê¸ˆÂ·ê²©ë¦¬ì ˆì°¨ì„œ":     "ìœ„í—˜ê¸°ê³„ì ê¸ˆÂ·ê²©ë¦¬ì ˆì°¨ì„œ",
    "ë°©í­ì„¤ë¹„ìœ ì§€ë³´ìˆ˜ê³„íšì„œ":     "ë°©í­ì„¤ë¹„ìœ ì§€ë³´ìˆ˜ê³„íšì„œ",
    "ë³´ê±´ê´€ë¦¬ìˆœíšŒì¼ì§€":           "ë³´ê±´ê´€ë¦¬ìˆœíšŒì¼ì§€",
    "ë¹„ìƒëŒ€ì‘í›ˆë ¨ê³„íšì„œ":         "ë¹„ìƒëŒ€ì‘í›ˆë ¨ê³„íšì„œ",
    "ì‚¬ë¬´ì‹¤ì•ˆì „ì ê²€í‘œ":           "ì‚¬ë¬´ì‹¤ì•ˆì „ì ê²€í‘œ",
    "ìƒì‚°ì„¤ë¹„ì •ë¹„ê³„íšì„œ":         "ìƒì‚°ì„¤ë¹„ì •ë¹„ê³„íšì„œ",
    "ì„ ë°•Â·í•´ì–‘êµ¬ì¡°ë¬¼ì ê²€í‘œ":       "ì„ ë°•Â·í•´ì–‘êµ¬ì¡°ë¬¼ì ê²€í‘œ",
    "ì†ŒìŒì§„ë™ì¸¡ì •ê³„íšì„œ":         "ì†ŒìŒì§„ë™ì¸¡ì •ê³„íšì„œ",
    "ì†Œë°©ì„¤ë¹„ì ê²€í‘œ":             "ì†Œë°©ì„¤ë¹„ì ê²€í‘œ",
    "ì•ˆì „êµìœ¡ê³„íšì„œ":             "ì•ˆì „êµìœ¡ê³„íšì„œ",
    "ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„êµ¬ì¶•ê³„íšì„œ":   "ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„êµ¬ì¶•ê³„íšì„œ",
    "ì•ˆì „ì‘ì—…í—ˆê°€ì ˆì°¨ì„œ":         "ì•ˆì „ì‘ì—…í—ˆê°€ì ˆì°¨ì„œ",
    "ì•ˆì „ì‘ì—…ì¼ì§€":               "ì•ˆì „ì‘ì—…ì¼ì§€",
    "ì‚°ì—…ì•ˆì „ë³´ê±´ìœ„ì›íšŒíšŒì˜ë¡":     "ì‚°ì—…ì•ˆì „ë³´ê±´ìœ„ì›íšŒíšŒì˜ë¡",
    "ì‚°ì—…ì¬í•´ì˜ˆë°©ê³„íšì„œ":         "ì‚°ì—…ì¬í•´ì˜ˆë°©ê³„íšì„œ",
    "ì‹œì„¤ë¬¼ìœ ì§€ê´€ë¦¬ê³„íšì„œ":       "ì‹œì„¤ë¬¼ìœ ì§€ê´€ë¦¬ê³„íšì„œ",
    "ìŠ¹ê°•ê¸°ì •ê¸°ê²€ì‚¬ê³„íšì„œ":       "ìŠ¹ê°•ê¸°ì •ê¸°ê²€ì‚¬ê³„íšì„œ",
    "ì•„ì´ì†Œê°€ìŠ¤ì¸¡ì •ê³„íšì„œ":       "ì•„ì´ì†Œê°€ìŠ¤ì¸¡ì •ê³„íšì„œ",
    "ì‘ì—…í—ˆê°€ì„œ":                 "ì‘ì—…í—ˆê°€ì„œ",
    "ì‘ì—…í™˜ê²½ì¸¡ì •ê³„íšì„œ":         "ì‘ì—…í™˜ê²½ì¸¡ì •ê³„íšì„œ",
    "ìœ„í—˜ì„±í‰ê°€ë§¤ë‰´ì–¼":           "ìœ„í—˜ì„±í‰ê°€ë§¤ë‰´ì–¼",
    "ìœ„í—˜ì„±í‰ê°€ë³´ê³ ì„œ":           "ìœ„í—˜ì„±í‰ê°€ë³´ê³ ì„œ",
    "ìœ„í—˜ìœ„í•´ë°©ì§€ê³„íšì„œ":         "ìœ„í—˜ìœ„í•´ë°©ì§€ê³„íšì„œ",
    "ì‘ê¸‰ì²˜ì¹˜í›ˆë ¨ê¸°ë¡í‘œ":         "ì‘ê¸‰ì²˜ì¹˜í›ˆë ¨ê¸°ë¡í‘œ",
    "ì¥ë¹„ê²€ì‚¬ê¸°ë¡í‘œ":             "ì¥ë¹„ê²€ì‚¬ê¸°ë¡í‘œ",
    "ì ê²€í‘œì‘ì„±ê°€ì´ë“œë¼ì¸":       "ì ê²€í‘œì‘ì„±ê°€ì´ë“œë¼ì¸",
    "ì¤‘ëŒ€ì‚¬ê³ ì¡°ì‚¬ë³´ê³ ì„œ":         "ì¤‘ëŒ€ì‚¬ê³ ì¡°ì‚¬ë³´ê³ ì„œ",
    "ì¶œì…í†µì œê´€ë¦¬ëŒ€ì¥":           "ì¶œì…í†µì œê´€ë¦¬ëŒ€ì¥",
    "í’ˆì§ˆì•ˆì „ë³´ì¦ê³„íšì„œ":         "í’ˆì§ˆì•ˆì „ë³´ì¦ê³„íšì„œ",
    "í™˜ê²½ì˜í–¥í‰ê°€ê³„íšì„œ":         "í™˜ê²½ì˜í–¥í‰ê°€ê³„íšì„œ",
    "í˜„ì¥ì•ˆì „ì ê²€í‘œ":             "í˜„ì¥ì•ˆì „ì ê²€í‘œ",
    "íšŒì „ê¸°ê³„ì ê²€ê³„íšì„œ":         "íšŒì „ê¸°ê³„ì ê²€ê³„íšì„œ",
    "íšŒì˜ë¡ì„œì‹(ì•ˆì „ë³´ê±´)":       "íšŒì˜ë¡ì„œì‹(ì•ˆì „ë³´ê±´)",
    "í™”í•™ë¬¼ì§ˆê´€ë¦¬ê³„íšì„œ":         "í™”í•™ë¬¼ì§ˆê´€ë¦¬ê³„íšì„œ",
}

def resolve_keyword(raw_keyword: str, template_list: List[str]) -> str:
    """
    1) KEYWORD_ALIAS ë§¤í•‘ ìš°ì„  ì ìš© (ê¸¸ì´ ìˆœ)
    2) difflibë¡œ fuzzy ë§¤ì¹­
    3) ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ë°˜í™˜(ì´í›„ 404 ì²˜ë¦¬)
    """
    # 1) alias (ê¸¸ì´ ìˆœìœ¼ë¡œ êµ¬ì²´ ë§¤í•‘ ë¨¼ì €)
    for alias in sorted(KEYWORD_ALIAS.keys(), key=len, reverse=True):
        if alias in raw_keyword:
            return KEYWORD_ALIAS[alias]

    # 2) fuzzy match
    cleaned = raw_keyword.replace(" ", "").lower()
    candidates = [t.replace(" ", "").lower() for t in template_list]
    matches = difflib.get_close_matches(cleaned, candidates, n=1, cutoff=0.6)
    if matches:
        idx = candidates.index(matches[0])
        return template_list[idx]

    # 3) no match
    return raw_keyword

@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸: /daily_news, /render_news, /create_xlsx", 200

# XLSX ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        return {"error": "í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)
    if "í…œí”Œë¦¿ëª…" not in df.columns:
        return {"error": "í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 500

    # fuzzy ë§¤ì¹­ ì ìš©
    template_list = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    tpl = resolve_keyword(raw, template_list)

    filtered = df[df["í…œí”Œë¦¿ëª…"].astype(str) == tpl]
    if filtered.empty:
        return {"error": f"'{tpl}' ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 404

    out_df = filtered[["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]]
    output = BytesIO()
    out_df.to_excel(output, index=False)
    output.seek(0)

    return send_file(
        output,
        as_attachment=True,
        download_name=f"{tpl}.xlsx",
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

# ì´í•˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë Œë” í•¨ìˆ˜ (ìˆ˜ì • ì—†ìŒ)

def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

def crawl_naver_news():
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        params = {"query": kw, "display": 2, "sort": "date"}
        resp = requests.get(base_url, headers=headers, params=params, timeout=10)
        if resp.status_code != 200:
            continue
        for item in resp.json().get("items", []):
            title = BeautifulSoup(item.get("title",""), "html.parser").get_text()
            desc  = BeautifulSoup(item.get("description",""), "html.parser").get_text()
            out.append({
                "ì¶œì²˜": item.get("originallink","ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": item.get("link",""),
                "ë‚ ì§œ": item.get("pubDate",""),
                "ë³¸ë¬¸": desc
            })
    return out

def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        resp = requests.get(f"{base}/search/news?searchword={kw}",
                            headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if resp.status_code != 200:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t    = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else None
            d    = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out

@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify({"error":"ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200
    return jsonify(news)

@app.route("/render_news", methods=["GET"])
def render_news():
    raw    = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in raw:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)
    news_items = sorted(filtered, key=lambda x: parser.parse(x["ë‚ ì§œ"]), reverse=True)[:3]
    if not news_items:
        return jsonify({"error":"ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200

    template_text = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{title}â€ ({date}, {source})\n\n"
        "{headline}\n"
        "ğŸ” {recommendation}\n"
        "ğŸ‘‰ ìš”ì•½ ì œê³µë¨ Â· â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ ì…ë ¥ ì‹œ ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€ í™•ì¸ ê°€ëŠ¥"
    )
    system_message = {
        "role":"system",
        "content":f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template_text}"
    }
    user_message = {"role":"user","content":str(news_items)}

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[system_message, user_message],
        max_tokens=800,
        temperature=0.7
    )
    return jsonify({"formatted_news": resp.choices[0].message.content})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

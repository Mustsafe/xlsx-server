from flask import Flask, request, jsonify, send_from_directory, Response
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import openai
import difflib
from dateutil import parser
from datetime import datetime, timedelta
from io import BytesIO
from typing import List
from urllib.parse import quote
import json
import logging

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # í•œê¸€ ê¹¨ì§ ë°©ì§€

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
openai.api_key = os.getenv("OPENAI_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ë°ì´í„° ë””ë ‰í† ë¦¬
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)

# í—¬ìŠ¤ì²´í¬
@app.route("/health", methods=["GET"])
def health_check():
    logger.info("Health check endpoint called")
    return "OK", 200

# í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸
@app.route("/.well-known/<path:filename>")
def serve_well_known(filename):
    logger.info(f"Serving well-known file: {filename}")
    return send_from_directory(
        os.path.join(app.root_path, "static", ".well-known"),
        filename, mimetype="application/json"
    )

@app.route("/openapi.json")
def serve_openapi():
    logger.info("Serving openapi.json")
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "openapi.json", mimetype="application/json"
    )

@app.route("/logo.png")
def serve_logo():
    logger.info("Serving logo.png")
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "logo.png", mimetype="image/png"
    )

# alias map ìƒì„±
def build_alias_map(template_list: List[str]) -> dict:
    alias = {}
    SUFFIXES = [" ì ê²€í‘œ", " ê³„íšì„œ", " ì„œì‹", " í‘œ", "ì–‘ì‹", " ì–‘ì‹", "_ì–‘ì‹"]
    for tpl in template_list:
        alias[tpl] = tpl
        alias[tpl.replace("_", " ")] = tpl
        alias[tpl.replace(" ", "_")] = tpl
        low = tpl.lower()
        alias[low] = tpl
        alias[low.replace("_", " ")] = tpl
        base_space = tpl.replace("_", " ")
        nospace = base_space.replace(" ", "").lower()
        alias[nospace] = tpl
        for suf in SUFFIXES:
            combo = base_space + suf
            alias[combo] = tpl
            alias[combo.replace(" ", "_")] = tpl
            alias[combo.lower()] = tpl
    for tpl in template_list:
        norm = tpl.lower().replace(" ", "").replace("_", "")
        if "jsa" in norm or "ì‘ì—…ì•ˆì „ë¶„ì„" in norm:
            alias["__FORCE_JSA__"] = tpl
        if "loto" in norm:
            alias["__FORCE_LOTO__"] = tpl
    temp = {}
    for k, v in alias.items():
        temp[k.replace(" ", "_")] = v
        temp[k.replace("_", " ")] = v
    alias.update(temp)
    return alias

# í‚¤ì›Œë“œ ë§¤í•‘
def resolve_keyword(raw_keyword: str, template_list: List[str], alias_map: dict) -> str:
    raw = raw_keyword.strip()
    norm = raw.replace("_", " ").replace("-", " ")
    key_lower = norm.lower()
    cleaned_key = key_lower.replace(" ", "")

    # JSA/LOTO ì˜ˆì™¸
    if "__FORCE_JSA__" in alias_map and ("jsa" in cleaned_key or "ì‘ì—…ì•ˆì „ë¶„ì„" in cleaned_key):
        return alias_map["__FORCE_JSA__"]
    if "__FORCE_LOTO__" in alias_map and "loto" in cleaned_key:
        return alias_map["__FORCE_LOTO__"]

    # 1) ì •í™• ì¼ì¹˜
    for tpl in template_list:
        if key_lower == tpl.lower() or cleaned_key == tpl.replace(" ", "").replace("_", "").lower():
            return tpl

    # 2) í† í° ë§¤ì¹˜ (ëª¨ë“  í† í°ì´ í¬í•¨ë  ë•Œë§Œ)
    tokens = [t for t in key_lower.split(" ") if t]
    candidates = [tpl for tpl in template_list if all(tok in tpl.lower() for tok in tokens)]
    if len(candidates) == 1:
        return candidates[0]

    # 3) alias ë§µ
    if raw in alias_map:
        return alias_map[raw]
    if key_lower in alias_map:
        return alias_map[key_lower]

    # 4) fuzzy ë§¤ì¹˜ (cutoff ë†’ì„)
    candidates_norm = [t.replace(" ", "").replace("_", "").lower() for t in template_list]
    matches = difflib.get_close_matches(cleaned_key, candidates_norm, n=1, cutoff=0.8)
    if matches:
        return template_list[candidates_norm.index(matches[0])]

    # ì‹¤íŒ¨ ì‹œ
    raise ValueError(f"í…œí”Œë¦¿ '{raw_keyword}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° endpoints: /health, /daily_news, /render_news, /create_xlsx, /list_templates", 200

@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    logger.info(f"create_xlsx called with template={raw}")

    # CSV ë¡œë“œ ë° ê¸°ë³¸ ê²€ì¦
    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        logger.error("í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404

    df = pd.read_csv(csv_path)
    if "í…œí”Œë¦¿ëª…" not in df.columns:
        logger.error("í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return jsonify(error="í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."), 500

    template_list = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    alias_map      = build_alias_map(template_list)

    # 1) ë“±ë¡ëœ í…œí”Œë¦¿ lookup
    try:
        tpl = resolve_keyword(raw, template_list, alias_map)
        logger.info(f"Template matched: {tpl}")
        filtered = df[df["í…œí”Œë¦¿ëª…"] == tpl]
        out_df   = filtered[["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]]
    # 2) ë¯¸ë“±ë¡ í…œí”Œë¦¿ì¼ ë•Œ GPTë¡œ fallback
    except ValueError:
        logger.warning(f"Template '{raw}' not found â†’ using GPT fallback")

        system_prompt = {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ ì‚°ì—…ì•ˆì „ ë¶„ì•¼ ë¬¸ì„œ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                "ì•„ë˜ ì»¬ëŸ¼ êµ¬ì¡°ì™€ ì‘ì„± ìŠ¤íƒ€ì¼ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬, "
                "ìš”ì²­ëœ í…œí”Œë¦¿ëª…ì´ ë“±ë¡ë˜ì–´ ìˆì§€ ì•Šì„ ë•Œ **5ê°œ ì´ìƒì˜** í•­ëª©ì„ ê°–ì¶˜ JSON ë°°ì—´ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n\n"
                "ì»¬ëŸ¼ êµ¬ì¡°:\n"
                "  â€¢ ì‘ì—… í•­ëª© (ì„¹ì…˜ ì œëª©)\n"
                "  â€¢ ì‘ì„± ì–‘ì‹ (ê°„ê²°Â·ëª…í™•í•œ ì‘ì„± ì§€ì¹¨)\n"
                "  â€¢ ì‹¤ë¬´ ì˜ˆì‹œ 1 (í˜„ì¥ í™œìš© ì˜ˆì‹œ)\n"
                "  â€¢ ì‹¤ë¬´ ì˜ˆì‹œ 2 (ì¶”ê°€ í™œìš© ì˜ˆì‹œ)\n\n"
                f"í…œí”Œë¦¿ëª…: {raw}\n"
            )
        }
        user_prompt = {
            "role": "user",
            "content": f"í…œí”Œë¦¿ëª… '{raw}'ì— ëŒ€í•œ ê¸°ë³¸ ì–‘ì‹ì„ JSONìœ¼ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”."
        }

        # GPT í˜¸ì¶œ (v1 ì¸í„°í˜ì´ìŠ¤)
        try:
            resp = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[system_prompt, user_prompt],
                max_tokens=800,
                temperature=0.5,
            )
            text = resp.choices[0].message.content

            # JSON íŒŒì‹± ì‹œë„
            try:
                data = json.loads(text)
                out_df = pd.DataFrame(data)
            except Exception as parse_err:
                logger.error(f"Fallback JSON parsing failed: {parse_err}\nContent: {text}")
                out_df = pd.DataFrame([{
                    "ì‘ì—… í•­ëª©": raw,
                    "ì‘ì„± ì–‘ì‹": text,
                    "ì‹¤ë¬´ ì˜ˆì‹œ 1": "",
                    "ì‹¤ë¬´ ì˜ˆì‹œ 2": ""
                }])
        except Exception as llm_err:
            logger.error(f"GPT call failed: {llm_err}")
            out_df = pd.DataFrame([{
                "ì‘ì—… í•­ëª©": raw,
                "ì‘ì„± ì–‘ì‹": "",
                "ì‹¤ë¬´ ì˜ˆì‹œ 1": "",
                "ì‹¤ë¬´ ì˜ˆì‹œ 2": ""
            }])

        # ê²°ê³¼ë¥¼ ê³ ë„í™”ëœ í‘œ í˜•ì‹ìœ¼ë¡œ ì—‘ì…€ ë³€í™˜í•˜ì—¬ ì‘ë‹µ (openpyxl ì‚¬ìš©)
    from openpyxl import Workbook
    from openpyxl.styles import Font

    wb = Workbook()
    ws = wb.active

    # 1) í—¤ë”
    ws.append(list(out_df.columns))
    for cell in ws[1]:
        cell.font = Font(bold=True)

    # 2) ë°ì´í„° í–‰
    for row in out_df.itertuples(index=False):
        ws.append(row)

    # 3) ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
    buffer = BytesIO()
    wb.save(buffer)
    buffer.seek(0)

    logger.info(f"Response ready for template={raw}")
    filename    = f"{tpl if 'tpl' in locals() else raw}.xlsx"
    disposition = "attachment; filename*=UTF-8''" + quote(filename)
    headers     = {
        "Content-Type":        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "Content-Disposition": disposition,
        "Cache-Control":       "public, max-age=3600"
    }
    return Response(buffer.read(), headers=headers)

@app.route("/list_templates", methods=["GET"])
def list_templates():
    logger.info("list_templates called")
    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        logger.error("í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404
    df = pd.read_csv(csv_path)
    return jsonify({
        "template_list": sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique()),
        "alias_keys": sorted(build_alias_map(sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique())).keys())
    })

# ë‰´ìŠ¤ í¬ë¡¤ë§ ìœ í‹¸ ë° ì—”ë“œí¬ì¸íŠ¸

def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp    = requests.get(url, headers=headers, timeout=10)
        soup    = BeautifulSoup(resp.text, "html.parser")
        node    = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

def crawl_naver_news():
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers  = {
        "X-Naver-Client-Id":     NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ",
                "í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        params = {"query": kw, "display": 2, "sort": "date"}
        resp   = requests.get(base_url, headers=headers, params=params, timeout=10)
        if resp.status_code != 200:
            continue
        for item in resp.json().get("items", []):
            title = BeautifulSoup(item.get("title",""), "html.parser").get_text()
            desc  = BeautifulSoup(item.get("description",""), "html.parser").get_text()
            out.append({
                "ì¶œì²˜": item.get("originallink","ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": item.get("link",""),
                "ë‚ ì§œ": item.get("pubDate",""),
                "ë³¸ë¬¸": desc
            })
    return out

def crawl_safetynews():
    base     = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ",
                "í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        resp = requests.get(f"{base}/search/news?searchword={kw}",
                            headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if resp.status_code != 200:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t    = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else None
            d    = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out

@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200
    return jsonify(news)

@app.route("/render_news", methods=["GET"])
def render_news():
    raw = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in raw:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)

    news_items = sorted(filtered,
                        key=lambda x: parser.parse(x["ë‚ ì§œ"]),
                        reverse=True)[:3]
    if not news_items:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200

    template_text = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{title}â€ ({date}, {ì¶œì²˜})\n\n"
        "{ë³¸ë¬¸}\n"
        "ğŸ” ë” ë³´ë ¤ë©´ â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )
    system_message = {
        "role":"system",
        "content":f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template_text}"
    }
    user_message = {"role":"user","content":str(news_items)}

    resp = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[system_prompt, user_prompt],
        max_tokens=500,
        temperature=0.5,
)

    return jsonify(formatted_news=resp.choices[0].message.content)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

from flask import Flask, request, send_file, jsonify
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

app = Flask(__name__)

# âœ… '/mnt/data' ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±
DATA_DIR = "/mnt/data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# âœ… ê¸°ì¡´ ì‘ì—…ê³„íšì„œ í‚¤ì›Œë“œ ë§¤í•‘
KEYWORD_ALIAS = {
    "ê³ ì†Œì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œ ì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…ëŒ€ ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œì‘ì—…": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„ ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ì •ì „ ì‘ì—… í—ˆê°€ì„œ": "ì •ì „ì‘ì—…í—ˆê°€ì„œ", "ì •ì „ì‘ì—…": "ì •ì „ì‘ì—…í—ˆê°€ì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "êµ¬ì¡°ë¬¼ í•´ì²´ ê³„íš": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ì‘ì—…": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ", "í¬ë ˆì¸ ì‘ì—… ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ê³ ì˜¨ ì‘ì—… í—ˆê°€ì„œ": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ", "ê³ ì˜¨ì‘ì—…": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ",
    "í™”ê¸°ì‘ì—… í—ˆê°€ì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸° ì‘ì—…ê³„íšì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸°ì‘ì—…": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—…ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸° ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸°ì‘ì—…": "ì „ê¸°ì‘ì—…ê³„íšì„œ",
    "êµ´ì°©ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì°©ê¸° ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì‚­ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ",
    "ìš©ì ‘ì‘ì—… ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ìš©ë‹¨ ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ì‘ì—…": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—… í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ê³ ì•• ì „ê¸°ì‘ì—… ê³„íšì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ì „ê¸° í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ì‘ì—…ê³„íš": "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ", "í˜‘ì°© ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ ì‘ì—… ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ", "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ", "ê³ ì••ê°€ìŠ¤ ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ"
}

TEMPLATES = {name: {"columns": ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ"], "drop_columns": []} for name in KEYWORD_ALIAS.values()}
SOURCES = {name: f"â€» ë³¸ ì–‘ì‹ì€ {name} ê´€ë ¨ ë²•ë ¹ ë˜ëŠ” ì§€ì¹¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤." for name in KEYWORD_ALIAS.values()}

def resolve_keyword(raw_keyword: str) -> str:
    for alias, standard in KEYWORD_ALIAS.items():
        if alias in raw_keyword:
            return standard
    return raw_keyword

# âœ… ì‘ì—…ê³„íšì„œ xlsx ìƒì„±
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw_template = request.args.get("template", "")
    template_name = resolve_keyword(raw_template)

    if not template_name or template_name not in TEMPLATES:
        return {"error": f"'{raw_template}'(ìœ¼)ë¡œëŠ” ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 400

    csv_path = os.path.join(DATA_DIR, f"{template_name}.csv")
    if not os.path.exists(csv_path):
        return {"error": "CSV ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)
    drop_cols = TEMPLATES[template_name].get("drop_columns", [])
    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors="ignore")

    final_cols = TEMPLATES[template_name]["columns"]
    df = df[[col for col in final_cols if col in df.columns]]

    if template_name in SOURCES:
        source_text = SOURCES[template_name]
        df.loc[len(df)] = [source_text] + [""] * (len(df.columns) - 1)

    xlsx_path = os.path.join(DATA_DIR, f"{template_name}_ìµœì¢…ì–‘ì‹.xlsx")
    df.to_excel(xlsx_path, index=False)

    return send_file(xlsx_path, as_attachment=True, download_name=f"{template_name}.xlsx")

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
def crawl_naver_news():
    base_url = "https://search.naver.com/search.naver"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ", "ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]

    headers = {"User-Agent": "Mozilla/5.0"}
    collected = []

    for keyword in keywords:
        params = {"where": "news", "query": keyword}
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        news_items = soup.select(".list_news > li")

        for item in news_items:
            title_tag = item.select_one(".news_tit")
            date_tag = item.select_one(".info_group span.date")

            if title_tag and title_tag.get("title") and title_tag.get("href"):
                collected.append({
                    "ì¶œì²˜": "ë„¤ì´ë²„",
                    "ì œëª©": title_tag["title"],
                    "ë§í¬": title_tag["href"],
                    "ë‚ ì§œ": date_tag.text.strip() if date_tag else ""
                })
    return collected

# âœ… ì•ˆì „ì‹ ë¬¸ í¬ë¡¤ë§
def crawl_safetynews():
    base_url = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ", "ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]

    headers = {"User-Agent": "Mozilla/5.0"}
    collected = []

    for keyword in keywords:
        search_url = f"{base_url}/search/news?searchword={keyword}"
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        news_items = soup.select(".article-list-content")

        for item in news_items:
            title_element = item.select_one(".list-titles")
            date_element = item.select_one(".list-dated")

            if title_element:
                title = title_element.text.strip()
                link = base_url + title_element.get("href")
                date = date_element.text.strip() if date_element else ""

                collected.append({
                    "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                    "ì œëª©": title,
                    "ë§í¬": link,
                    "ë‚ ì§œ": date
                })
    return collected

# âœ… í†µí•© ë‰´ìŠ¤ í¬ë¡¤ë§
@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    try:
        # ğŸ›  ì—¬ê¸° ì¶”ê°€ (í•µì‹¬)
        if not os.path.exists(DATA_DIR):
            os.makedirs(DATA_DIR)

        naver_news = crawl_naver_news()
        safety_news = crawl_safetynews()

        all_news = naver_news + safety_news

        # ìµœê·¼ 7ì¼ í•„í„°
        today = datetime.now()
        one_week_ago = today - timedelta(days=7)

        filtered_news = []
        for news in all_news:
            try:
                date = datetime.strptime(news["ë‚ ì§œ"], "%Y.%m.%d.")
                if one_week_ago <= date <= today:
                    filtered_news.append(news)
            except:
                continue

        if not filtered_news:
            return {"error": "ìµœê·¼ 7ì¼ ë‚´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}, 200

        df = pd.DataFrame(filtered_news)
        filename = os.path.join(DATA_DIR, f"daily_safety_news_{today.strftime('%Y%m%d')}.csv")
        df.to_csv(filename, index=False, encoding="utf-8-sig")

        return send_file(filename, as_attachment=True)

    except Exception as e:
        return {"error": f"Internal Server Error: {str(e)}"}, 500

# âœ… ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

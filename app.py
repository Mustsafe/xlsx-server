from flask import Flask, request, send_file, jsonify
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

app = Flask(__name__)

# ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# ğŸ”‘ ì‘ì—…ê³„íšì„œ í‚¤ì›Œë“œ ë§¤í•‘
KEYWORD_ALIAS = {
    "ê³ ì†Œì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œ ì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…ëŒ€ ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œì‘ì—…": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„ ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ì •ì „ ì‘ì—… í—ˆê°€ì„œ": "ì •ì „ì‘ì—…í—ˆê°€ì„œ", "ì •ì „ì‘ì—…": "ì •ì „ì‘ì—…í—ˆê°€ì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "êµ¬ì¡°ë¬¼ í•´ì²´ ê³„íš": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ì‘ì—…": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ", "í¬ë ˆì¸ ì‘ì—… ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ê³ ì˜¨ ì‘ì—… í—ˆê°€ì„œ": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ", "ê³ ì˜¨ì‘ì—…": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ",
    "í™”ê¸°ì‘ì—… í—ˆê°€ì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸° ì‘ì—…ê³„íšì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸°ì‘ì—…": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—…ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸° ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸°ì‘ì—…": "ì „ê¸°ì‘ì—…ê³„íšì„œ",
    "êµ´ì°©ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì°©ê¸° ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì‚­ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ",
    "ìš©ì ‘ì‘ì—… ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ìš©ë‹¨ ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ì‘ì—…": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—… í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ê³ ì•• ì „ê¸°ì‘ì—… ê³„íšì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ì „ê¸° í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ì‘ì—…ê³„íš": "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ", "í˜‘ì°© ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ ì‘ì—… ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ", "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ", "ê³ ì••ê°€ìŠ¤ ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ"
}

# ğŸ“‹ ì—‘ì…€ í…œí”Œë¦¿ ì„¤ì •
TEMPLATES = {
    name: {"columns": ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ"], "drop_columns": []}
    for name in KEYWORD_ALIAS.values()
}
SOURCES = {
    name: f"â€» ë³¸ ì–‘ì‹ì€ {name} ê´€ë ¨ ë²•ë ¹ ë˜ëŠ” ì§€ì¹¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    for name in KEYWORD_ALIAS.values()
}

def resolve_keyword(raw_keyword: str) -> str:
    for alias, standard in KEYWORD_ALIAS.items():
        if alias in raw_keyword:
            return standard
    return raw_keyword

# ğŸš€ ì‘ì—…ê³„íšì„œ XLSX ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw_template = request.args.get("template", "")
    template_name = resolve_keyword(raw_template)

    if not template_name or template_name not in TEMPLATES:
        return {"error": f"'{raw_template}'(ìœ¼)ë¡œëŠ” ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 400

    csv_path = os.path.join(DATA_DIR, f"{template_name}.csv")
    if not os.path.exists(csv_path):
        return {"error": "CSV ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)
    drop_cols = TEMPLATES[template_name].get("drop_columns", [])
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
    final_cols = TEMPLATES[template_name]["columns"]
    df = df[[c for c in final_cols if c in df.columns]]

    # ì¶œì²˜ í–‰ ì¶”ê°€
    source_text = SOURCES.get(template_name)
    if source_text:
        df.loc[len(df)] = [source_text] + ["" for _ in range(len(df.columns)-1)]

    xlsx_path = os.path.join(DATA_DIR, f"{template_name}_ìµœì¢…ì–‘ì‹.xlsx")
    df.to_excel(xlsx_path, index=False)
    return send_file(xlsx_path, as_attachment=True, download_name=f"{template_name}.xlsx")

# ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ìœ í‹¸
 def fetch_naver_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        if soup.select_one("div#dic_area"):
            return soup.select_one("div#dic_area").get_text(separator="\n").strip()
        if soup.select_one("article"):
            return soup.select_one("article").get_text(separator="\n").strip()
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

 def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        if soup.select_one("div#article-view-content-div"):
            return soup.select_one("div#article-view-content-div").get_text(separator="\n").strip()
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

# ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ìµœì‹  2ê°œ)
 def crawl_naver_news():
    base = "https://search.naver.com/search.naver"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    collected = []
    for kw in keywords:
        params = {"where":"news","query":kw}
        r = requests.get(base, params=params, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code!=200: continue
        soup = BeautifulSoup(r.text, "html.parser")
        items = soup.select(".list_news > li")[:2]
        for it in items:
            t = it.select_one(".news_tit")
            if not t or not t.get('href'): continue
            link = t['href']; title=t.get('title','')
            date_tag = it.select_one(".info_group span.date")
            date_text = date_tag.text.strip() if date_tag else ''
            body = fetch_naver_article_content(link)
            collected.append({"ì¶œì²˜":"ë„¤ì´ë²„","ì œëª©":title,"ë§í¬":link,"ë‚ ì§œ":date_text,"ë³¸ë¬¸":body[:2000]})
    return collected

# ğŸ“° ì•ˆì „ì‹ ë¬¸ í¬ë¡¤ëŸ¬ (ìµœì‹  2ê°œ)
 def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    collected = []
    for kw in keywords:
        url = f"{base}/search/news?searchword={kw}"
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code!=200: continue
        soup = BeautifulSoup(r.text, "html.parser")
        items = soup.select(".article-list-content")[:2]
        for it in items:
            title_el = it.select_one(".list-titles")
            if not title_el or not title_el.get('href'): continue
            link = base+title_el['href']; title=title_el.text.strip()
            date_el = it.select_one(".list-dated")
            date_text = date_el.text.strip() if date_el else ''
            body = fetch_safetynews_article_content(link)
            collected.append({"ì¶œì²˜":"ì•ˆì „ì‹ ë¬¸","ì œëª©":title,"ë§í¬":link,"ë‚ ì§œ":date_text,"ë³¸ë¬¸":body[:2000]})
    return collected

# ğŸŒ í†µí•© ë‰´ìŠ¤ API
@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    try:
        naver = crawl_naver_news()
        safety = crawl_safetynews()
        all_news = naver + safety
        if not all_news:
            return {"error":"ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}, 200
        return jsonify(all_news)
    except Exception as e:
        return {"error":f"Internal Server Error: {str(e)}"}, 500

# â–¶ï¸ ì•± ì‹¤í–‰
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

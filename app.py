from flask import Flask, request, send_file, jsonify
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

app = Flask(__name__)

# ğŸ“‚ ë°ì´í„° ë””ë ‰í† ë¦¬
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# ğŸ”‘ ì‘ì—…ê³„íšì„œ í‚¤ì›Œë“œ ë§¤í•‘
KEYWORD_ALIAS = {
    "ê³ ì†Œì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œ ì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…ëŒ€ ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œì‘ì—…": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„ ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ì •ì „ ì‘ì—… í—ˆê°€ì„œ": "ì •ì „ì‘ì—…í—ˆê°€ì„œ", "ì •ì „ì‘ì—…": "ì •ì „ì‘ì—…í—ˆê°€ì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "êµ¬ì¡°ë¬¼ í•´ì²´ ê³„íš": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ì‘ì—…": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ", "í¬ë ˆì¸ ì‘ì—… ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ê³ ì˜¨ ì‘ì—… í—ˆê°€ì„œ": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ", "ê³ ì˜¨ì‘ì—…": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ",
    "í™”ê¸°ì‘ì—… í—ˆê°€ì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸° ì‘ì—…ê³„íšì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸°ì‘ì—…": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—…ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸° ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸°ì‘ì—…": "ì „ê¸°ì‘ì—…ê³„íšì„œ",
    "êµ´ì°©ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì°©ê¸° ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì‚­ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ",
    "ìš©ì ‘ì‘ì—… ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ìš©ë‹¨ ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ì‘ì—…": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—… í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ê³ ì•• ì „ê¸°ì‘ì—… ê³„íšì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ì „ê¸° í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ì‘ì—…ê³„íš": "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ", "í˜‘ì°© ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ ì‘ì—… ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ", "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ", "ê³ ì••ê°€ìŠ¤ ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ"
}

# ì—‘ì…€ í…œí”Œë¦¿ ì„¤ì •
TEMPLATES = {
    name: {"columns": ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ"], "drop_columns": []}
    for name in KEYWORD_ALIAS.values()
}
SOURCES = {
    name: f"â€» ë³¸ ì–‘ì‹ì€ {name} ê´€ë ¨ ë²•ë ¹ ë˜ëŠ” ì§€ì¹¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    for name in KEYWORD_ALIAS.values()
}

def resolve_keyword(raw_keyword: str) -> str:
    for alias, standard in KEYWORD_ALIAS.items():
        if alias in raw_keyword:
            return standard
    return raw_keyword

# â–¶ï¸ ì‘ì—…ê³„íšì„œ ì—‘ì…€ ìƒì„±
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    tmpl = resolve_keyword(raw)
    if tmpl not in TEMPLATES:
        return {"error": f"'{raw}'ë¡œëŠ” ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 400
    src = os.path.join(DATA_DIR, f"{tmpl}.csv")
    if not os.path.exists(src):
        return {"error": "CSV íŒŒì¼ ì—†ìŒ"}, 404
    df = pd.read_csv(src)
    drops = TEMPLATES[tmpl]["drop_columns"]
    df = df.drop(columns=[c for c in drops if c in df.columns], errors="ignore")
    cols = TEMPLATES[tmpl]["columns"]
    df = df[[c for c in cols if c in df.columns]]
    if tmpl in SOURCES:
        df.loc[len(df)] = [SOURCES[tmpl]] + [""]*(len(df.columns)-1)
    out = os.path.join(DATA_DIR, f"{tmpl}_final.xlsx")
    df.to_excel(out, index=False)
    return send_file(out, as_attachment=True, download_name=f"{tmpl}.xlsx")

# â–¶ï¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
 def fetch_naver_article_content(url):
    try:
        h = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=h, timeout=10)
        s = BeautifulSoup(r.text, "html.parser")
        if s.select_one("div#dic_area"):
            return s.select_one("div#dic_area").get_text(separator="\n").strip()
        if s.select_one("article"):
            return s.select_one("article").get_text(separator="\n").strip()
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

def fetch_safetynews_article_content(url):
    try:
        h = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=h, timeout=10)
        s = BeautifulSoup(r.text, "html.parser")
        div = s.select_one("div#article-view-content-div")
        return div.get_text(separator="\n").strip() if div else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

# â–¶ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    try:
        def crawl_naver():
            base = "https://search.naver.com/search.naver"
            kws = ["ê±´ì„¤ ì‚¬ê³ ","ì‚°ì—…ì•ˆì „"]
            res = []
            for kw in kws:
                r = requests.get(base, params={"where":"news","query":kw}, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
                if r.status_code!=200: continue
                soup = BeautifulSoup(r.text, "html.parser")
                for li in soup.select(".list_news li")[:2]:
                    t = li.select_one(".news_tit")
                    if not t: continue
                    url = t["href"]
                    res.append({"ì¶œì²˜":"ë„¤ì´ë²„","ì œëª©":t.get("title",""),"ë§í¬":url,
                                "ë³¸ë¬¸":fetch_naver_article_content(url)})
            return res
        def crawl_safe():
            base = "https://www.safetynews.co.kr"
            res=[]
            for kw in ["ê±´ì„¤ ì‚¬ê³ ","ì‚°ì—…ì•ˆì „"]:
                r = requests.get(f"{base}/search/news?searchword={kw}", headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
                if r.status_code!=200: continue
                soup=BeautifulSoup(r.text,"html.parser")
                for it in soup.select(".article-list-content")[:2]:
                    a=it.select_one(".list-titles")
                    if not a: continue
                    url=base+a.get("href")
                    res.append({"ì¶œì²˜":"ì•ˆì „ì‹ ë¬¸","ì œëª©":a.text.strip(),"ë§í¬":url,
                                "ë³¸ë¬¸":fetch_safetynews_article_content(url)})
            return res
        data = crawl_naver()+crawl_safe()
        return jsonify(data)
    except Exception as e:
        return {"error":str(e)},500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

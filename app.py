from flask import Flask, request, send_file, jsonify
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import openai

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # í•œê¸€ ê¹¨ì§ ë°©ì§€

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai.api_key = os.getenv("OPENAI_API_KEY")

# ./data ë””ë ‰í† ë¦¬ ì‚¬ìš©
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# ë„¤ì´ë²„ ì˜¤í”ˆ API ìê²©ì¦ëª… (ì‹¤ì œë¡  í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬)
NAVER_CLIENT_ID     = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ì‘ì—…ê³„íšì„œ í‚¤ì›Œë“œ ë§¤í•‘ (ê¸°ì¡´ì— ì“°ì‹œë˜ ì „ì²´ ë§¤í•‘ ê·¸ëŒ€ë¡œ)
KEYWORD_ALIAS = {
    "ê³ ì†Œì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œ ì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…ëŒ€ ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ", "ê³ ì†Œì‘ì—…": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„ ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ì‘ì—… ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ", "ë°€íê³µê°„": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ì •ì „ ì‘ì—… í—ˆê°€ì„œ": "ì •ì „ì‘ì—…í—ˆê°€ì„œ", "ì •ì „ì‘ì—…": "ì •ì „ì‘ì—…í—ˆê°€ì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "êµ¬ì¡°ë¬¼ í•´ì²´ ê³„íš": "í•´ì²´ì‘ì—…ê³„íšì„œ", "í•´ì²´ì‘ì—…": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ", "í¬ë ˆì¸ ì‘ì—… ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ê³ ì˜¨ ì‘ì—… í—ˆê°€ì„œ": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ", "ê³ ì˜¨ì‘ì—…": "ê³ ì˜¨ì‘ì—…í—ˆê°€ì„œ",
    "í™”ê¸°ì‘ì—… í—ˆê°€ì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸° ì‘ì—…ê³„íšì„œ": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ", "í™”ê¸°ì‘ì—…": "í™”ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—…ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸° ê³„íšì„œ": "ì „ê¸°ì‘ì—…ê³„íšì„œ", "ì „ê¸°ì‘ì—…": "ì „ê¸°ì‘ì—…ê³„íšì„œ",
    "êµ´ì°©ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì°©ê¸° ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ", "êµ´ì‚­ê¸° ì‘ì—…ê³„íšì„œ": "êµ´ì°©ê¸°ì‘ì—…ê³„íšì„œ",
    "ìš©ì ‘ì‘ì—… ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ìš©ë‹¨ ê³„íšì„œ": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ", "ìš©ì ‘ì‘ì—…": "ìš©ì ‘ìš©ë‹¨ì‘ì—…í—ˆê°€ì„œ",
    "ì „ê¸° ì‘ì—… í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ê³ ì•• ì „ê¸°ì‘ì—… ê³„íšì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ", "ì „ê¸° í—ˆê°€ì„œ": "ì „ê¸°ì‘ì—…í—ˆê°€ì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ", "ë¹„ê³„ì‘ì—…ê³„íš": "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ", "í˜‘ì°© ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ ì‘ì—… ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ", "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "ì–‘ì¤‘ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ", "ê³ ì••ê°€ìŠ¤ ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ"
}

TEMPLATES = {
    name: {"columns": ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ"], "drop_columns": []}
    for name in KEYWORD_ALIAS.values()
}
SOURCES = {
    name: f"â€» ë³¸ ì–‘ì‹ì€ {name} ê´€ë ¨ ë²•ë ¹ ë˜ëŠ” ì§€ì¹¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    for name in KEYWORD_ALIAS.values()
}

def resolve_keyword(raw_keyword: str) -> str:
    for alias, std in KEYWORD_ALIAS.items():
        if alias in raw_keyword:
            return std
    return raw_keyword

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ XLSX ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    tpl = resolve_keyword(raw)
    if tpl not in TEMPLATES:
        return {"error": f"'{raw}' ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 400

    csv_path = os.path.join(DATA_DIR, f"{tpl}.csv")
    if not os.path.exists(csv_path):
        return {"error": "CSV ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)
    df = df.drop(columns=TEMPLATES[tpl]["drop_columns"], errors="ignore")
    df = df[[c for c in TEMPLATES[tpl]["columns"] if c in df.columns]]

    source = SOURCES.get(tpl)
    if source:
        df.loc[len(df)] = [source] + [""] * (len(df.columns) - 1)

    xlsx_path = os.path.join(DATA_DIR, f"{tpl}_ìµœì¢…ì–‘ì‹.xlsx")
    df.to_excel(xlsx_path, index=False)
    return send_file(xlsx_path, as_attachment=True, download_name=f"{tpl}.xlsx")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SafetyNews ë³¸ë¬¸ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë„¤ì´ë²„ ë‰´ìŠ¤ Open API í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_naver_news():
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    keywords = [
        "ê±´ì„¤ ì‚¬ê³ ", "ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ",
        "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"
    ]
    out = []
    for kw in keywords:
        params = {"query": kw, "display": 2, "sort": "date"}
        resp = requests.get(base_url, headers=headers, params=params, timeout=10)
        if resp.status_code != 200:
            continue
        for item in resp.json().get("items", []):
            title = BeautifulSoup(item.get("title",""), "html.parser").get_text()
            desc  = BeautifulSoup(item.get("description",""), "html.parser").get_text()
            link  = item.get("link","")
            pub   = item.get("pubDate","")  # RFC1123
            out.append({
                "ì¶œì²˜": item.get("originallink","ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": link,
                "ë‚ ì§œ": pub,
                "ë³¸ë¬¸": desc
            })
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SafetyNews í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    keywords = [
        "ê±´ì„¤ ì‚¬ê³ ", "ê±´ì„¤ ì‚¬ë§ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ",
        "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"
    ]
    out = []
    for kw in keywords:
        resp = requests.get(f"{base}/search/news?searchword={kw}",
                            headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        if resp.status_code != 200:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t    = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else None
            d    = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‚ ì§œ íŒŒì‹± ë„ìš°ë¯¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_date(date_str: str):
    for fmt in ("%Y.%m.%d", "%a, %d %b %Y %H:%M:%S %z"):
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPTë¡œ ë‰´ìŠ¤ í¬ë§·íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/render_news", methods=["GET"])
def render_news():
    # 1) í¬ë¡¤ë§
    news = crawl_naver_news() + crawl_safetynews()

    # 2) ìµœê·¼ 3ì¼ì¹˜ í•„í„°
    cutoff = datetime.now() - timedelta(days=3)
    recent = []
    for i in news:
        dt = parse_date(i.get("ë‚ ì§œ",""))
        if dt and dt >= cutoff:
            i["_dt"] = dt
            recent.append(i)
    if not recent:
        return jsonify({"error": "ìµœê·¼ 3ì¼ ë‚´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200

    # 3) ìµœì‹ ìˆœ ìƒìœ„ 3ê°œ
    top3 = sorted(recent, key=lambda x: x["_dt"], reverse=True)[:3]

    # 4) GPT í˜¸ì¶œ
    template_txt = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{ì œëª©}â€ ({ë‚ ì§œ}, {ì¶œì²˜})\n\n"
        "{ë³¸ë¬¸}\n"
        "ğŸ” {ì¶”ì²œë©”ì‹œì§€}\n"
        "ğŸ‘‰ ìš”ì•½ ì œê³µë¨ Â· â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ ì…ë ¥ ì‹œ ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€ í™•ì¸ ê°€ëŠ¥"
    )
    sys_msg = {
        "role": "system",
        "content": "ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ìœ„ í…œí”Œë¦¿ì— ë§ì¶° 3ê°œ í•­ëª©ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n" + template_txt
    }
    user_msg = {"role": "user", "content": str(top3)}

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[sys_msg, user_msg],
        max_tokens=800,
        temperature=0.7
    )
    return jsonify({"formatted_news": resp.choices[0].message.content})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

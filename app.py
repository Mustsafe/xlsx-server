from flask import Flask, request, send_file, jsonify, send_from_directory, Response
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import openai
import difflib
from dateutil import parser
from datetime import datetime, timedelta
from io import BytesIO
from typing import List
from itertools import product

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # í•œê¸€ ê¹¨ì§ ë°©ì§€

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai.api_key = os.getenv("OPENAI_API_KEY")

# ./data ë””ë ‰í† ë¦¬ ì‚¬ìš©
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- 1. í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ ---
@app.route("/health", methods=["GET"])
def health_check():
    return "OK", 200

# í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì„œë¹™
@app.route("/.well-known/<path:filename>")
def serve_well_known(filename):
    return send_from_directory(
        os.path.join(app.root_path, "static", ".well-known"),
        filename,
        mimetype="application/json"
    )

# OpenAPI ë° ë¡œê³  íŒŒì¼ ì„œë¹™
@app.route("/openapi.json")
def serve_openapi():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "openapi.json",
        mimetype="application/json"
    )

@app.route("/logo.png")
def serve_logo():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "logo.png",
        mimetype="image/png"
    )

# ë„¤ì´ë²„ ì˜¤í”ˆ API ìê²©ì¦ëª…
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

def build_alias_map(template_list: List[str]) -> dict:
    """
    template_list ì— ìˆëŠ” ê° í…œí”Œë¦¿ëª…ì— ëŒ€í•´
    ë‹¤ì–‘í•œ ë³€í˜•(alias)ì„ ìë™ ìƒì„±í•˜ì—¬ ë§¤í•‘ dict ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    alias = {}
    for tpl in template_list:
        # 1) ì›ë˜ ì´ë¦„
        alias[tpl] = tpl
        # 2) ì–¸ë”ìŠ¤ì½”ì–´ â†” ê³µë°±
        alias[tpl.replace("_", " ")] = tpl
        alias[tpl.replace(" ", "_")] = tpl
        # 3) ì†Œë¬¸ì ë²„ì „
        low = tpl.lower()
        alias[low] = tpl
        alias[low.replace("_", " ")] = tpl
        # 4) ì£¼ìš” ì ‘ë¯¸ì‚¬ ì¶”ê°€
        base_space = tpl.replace("_", " ")
        for suf in [" ì ê²€í‘œ", " ê³„íšì„œ", " ì„œì‹", " í‘œ"]:
            combo = base_space + suf
            alias[combo] = tpl
            alias[combo.replace(" ", "_")] = tpl
            alias[combo.lower()] = tpl
    return alias

def resolve_keyword(raw_keyword: str, template_list: List[str], alias_map: dict) -> str:
    """
    1) í† í° ê¸°ë°˜ ë§¤ì¹­: raw_keywordë¥¼ ë¶„ë¦¬í•œ í† í°ì´ tplì— ëª¨ë‘ í¬í•¨ë˜ë©´ ë°”ë¡œ ë§¤ì¹˜
    2) alias_map ë§¤í•‘ ìš°ì„  ì ìš©
    3) difflibë¡œ fuzzy ë§¤ì¹­ (ì–¸ë”ìŠ¤ì½”ì–´Â·ê³µë°± ëª¨ë‘ ì œê±°)
    4) ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ë°˜í™˜(ì´í›„ fallback ì²˜ë¦¬)
    """
    key = raw_keyword.strip()
    # 1) í† í° ê¸°ë°˜ ë§¤ì¹­
    tokens = [t for t in key.replace("_", " ").split(" ") if t]
    candidates = [tpl for tpl in template_list
                  if all(tok in tpl for tok in tokens)]
    if len(candidates) == 1:
        return candidates[0]

    # 2) alias ë§µ
    if key in alias_map:
        return alias_map[key]

    # 3) fuzzy match (ì–¸ë”ìŠ¤ì½”ì–´Â·ê³µë°± ëª¨ë‘ ì œê±°)
    cleaned = key.replace(" ", "").replace("_", "").lower()
    candidates_norm = [t.replace(" ", "").replace("_", "").lower() for t in template_list]
    matches = difflib.get_close_matches(cleaned, candidates_norm, n=1, cutoff=0.6)
    if matches:
        idx = candidates_norm.index(matches[0])
        return template_list[idx]

    # 4) no match
    return key

@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸: /health, /daily_news, /render_news, /create_xlsx", 200

# XLSX ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (ìŠ¤íŠ¸ë¦¬ë° ë° ìºì‹± í—¤ë” ì¶”ê°€)
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        return {"error": "í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)
    if "í…œí”Œë¦¿ëª…" not in df.columns:
        return {"error": "í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 500

    # í…œí”Œë¦¿ ëª©ë¡ ë° alias_map ìƒì„±
    template_list = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    alias_map = build_alias_map(template_list)

    # í‚¤ì›Œë“œ í•´ì„
    tpl = resolve_keyword(raw, template_list, alias_map)

    # í•„í„°ë§ ë° fallback ì²˜ë¦¬
    filtered = df[df["í…œí”Œë¦¿ëª…"].astype(str) == tpl]
    if filtered.empty:
        filtered = df[df["í…œí”Œë¦¿ëª…"] == template_list[0]]
        used_tpl = template_list[0]
    else:
        used_tpl = tpl

    out_df = filtered[["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]]

    # ìŠ¤íŠ¸ë¦¬ë° Response
    def generate_xlsx():
        buffer = BytesIO()
        out_df.to_excel(buffer, index=False)
        buffer.seek(0)
        while True:
            chunk = buffer.read(8192)
            if not chunk:
                break
            yield chunk

    headers = {
        "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "Content-Disposition": f'attachment; filename="{used_tpl}.xlsx"',
        "Cache-Control": "public, max-age=3600"
    }
    return Response(generate_xlsx(), headers=headers)

# ì´í•˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë Œë” í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

def crawl_naver_news():
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        params = {"query": kw, "display": 2, "sort": "date"}
        resp = requests.get(base_url, headers=headers, params=params, timeout=10)
        if resp.status_code != 200:
            continue
        for item in resp.json().get("items", []):
            title = BeautifulSoup(item.get("title",""), "html.parser").get_text()
            desc  = BeautifulSoup(item.get("description",""), "html.parser").get_text()
            out.append({
                "ì¶œì²˜": item.get("originallink","ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": item.get("link",""),
                "ë‚ ì§œ": item.get("pubDate",""),
                "ë³¸ë¬¸": desc
            })
    return out

def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ","ì¶”ë½ ì‚¬ê³ ","ë¼ì„ ì‚¬ê³ ","ì§ˆì‹ ì‚¬ê³ ","í­ë°œ ì‚¬ê³ ","ì‚°ì—…ì¬í•´","ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        resp = requests.get(f"{base}/search/news?searchword={kw}",
                            headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if resp.status_code != 200:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t    = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else None
            d    = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out

@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify({"error":"ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200
    return jsonify(news)

@app.route("/render_news", methods=["GET"])
def render_news():
    raw    = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in raw:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)

    news_items = sorted(filtered, key=lambda x: parser.parse(x["ë‚ ì§œ"]), reverse=True)[:3]
    if not news_items:
        return jsonify({"error":"ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200

    template_text = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{title}â€ ({date}, {source})\n\n"
        "{headline}\n"
        "ğŸ” {recommendation}\n"
        "ğŸ‘‰ ìš”ì•½ ì œê³µë¨ Â· â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ ì…ë ¥ ì‹œ ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€ í™•ì¸ ê°€ëŠ¥"
    )
    system_message = {
        "role":"system",
        "content":f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template_text}"
    }
    user_message = {"role":"user","content":str(news_items)}

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[system_message, user_message],
        max_tokens=800,
        temperature=0.7
    )
    return jsonify({"formatted_news": resp.choices[0].message.content})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

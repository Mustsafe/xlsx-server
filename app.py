from flask import Flask, request, jsonify, Response, send_from_directory
import pandas as pd
import os
import re
import json
import difflib
import requests
from bs4 import BeautifulSoup
from io import BytesIO
from typing import List
from urllib.parse import quote
from datetime import datetime, timedelta
from dateutil import parser
import openai

# â”€â”€ ì—‘ì…€ ìƒì„±ìš© import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
from openpyxl.utils import get_column_letter

# â”€â”€ ì•± ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False

openai.api_key     = os.getenv("OPENAI_API_KEY")
NAVER_CLIENT_ID     = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)


# â”€â”€ alias_map ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_alias_map(template_list: List[str]) -> dict:
    alias = {}
    SUFFIXES = [" ì ê²€í‘œ", " ê³„íšì„œ", " ì„œì‹", " í‘œ", "ì–‘ì‹", " ì–‘ì‹", "_ì–‘ì‹"]
    for tpl in template_list:
        base = tpl.replace("_", " ")
        low  = tpl.lower()
        nosp = base.replace(" ", "").lower()
        # ê¸°ë³¸ í‚¤
        keys = {tpl, base, low, nosp, tpl.replace(" ", "_"), low.replace("_", " ")}
        for k in keys:
            alias[k] = tpl
        # ì ‘ë¯¸ì‚¬ í‚¤
        for suf in SUFFIXES:
            for k in {base + suf, (base + suf).lower(), (base + suf).replace(" ", "_")}:
                alias[k] = tpl
    # JSA/LOTO ê°•ì œ ë§¤í•‘
    for tpl in template_list:
        norm = tpl.lower().replace(" ", "").replace("_", "")
        if "jsa" in norm or "ì‘ì—…ì•ˆì „ë¶„ì„" in norm:
            alias["__FORCE_JSA__"] = tpl
        if "loto" in norm:
            alias["__FORCE_LOTO__"] = tpl
    # ê³µë°±/ì–¸ë”ë°” í˜¼ìš© ì¶”ê°€
    extra = {}
    for k, v in alias.items():
        extra[k.replace(" ", "_")] = v
        extra[k.replace("_", " ")] = v
    alias.update(extra)
    return alias


# â”€â”€ í‚¤ì›Œë“œ â†’ í…œí”Œë¦¿ëª… resolve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def resolve_keyword(raw: str, templates: List[str], alias_map: dict) -> str:
    r = raw.strip()
    norm    = r.replace("_", " ").replace("-", " ").lower()
    cleaned = norm.replace(" ", "")

    # 1) JSA/LOTO ê°•ì œ
    if "__FORCE_JSA__" in alias_map and ("jsa" in cleaned or "ì‘ì—…ì•ˆì „ë¶„ì„" in cleaned):
        return alias_map["__FORCE_JSA__"]
    if "__FORCE_LOTO__" in alias_map and "loto" in cleaned:
        return alias_map["__FORCE_LOTO__"]

    # 2) ì§ì ‘ alias_map lookup
    for key in (r, norm, cleaned):
        if key in alias_map:
            return alias_map[key]

    # 3) single-token special: â€œlotoâ€ ë˜ëŠ” â€œjsaâ€ë§Œ ë“¤ì–´ì™”ì„ ë•Œ
    if cleaned in ("loto", "jsa", "ì‘ì—…ì•ˆì „ë¶„ì„"):
        for tpl in templates:
            tpl_norm = tpl.lower().replace(" ", "").replace("_", "")
            if cleaned in tpl_norm:
                return tpl

    # 4) prefix ë§¤ì¹­: cleanedê°€ tpl_norm ì ‘ë‘ì–´ì¼ ë•Œ
    prefix_cands = [
        tpl for tpl in templates
        if tpl.lower().replace(" ", "").replace("_", "").startswith(cleaned)
    ]
    if len(prefix_cands) == 1:
        return prefix_cands[0]

    # 5) í† í° ë§¤ì¹­ (ëª¨ë“  í† í° í¬í•¨)
    tokens = [t for t in norm.split() if t]
    tok_cands = [
        tpl for tpl in templates
        if all(tok in tpl.lower() for tok in tokens)
    ]
    if len(tok_cands) == 1:
        return tok_cands[0]

    # 6) ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
    substr_cands = [
        tpl for tpl in templates
        if cleaned in tpl.lower().replace(" ", "").replace("_", "")
    ]
    if len(substr_cands) == 1:
        return substr_cands[0]

    # 7) fuzzy ë§¤ì¹­
    norms = [t.replace(" ", "").replace("_", "").lower() for t in templates]
    matches = difflib.get_close_matches(cleaned, norms, n=1, cutoff=0.6)
    if matches:
        return templates[norms.index(matches[0])]

    # 8) ì—†ìœ¼ë©´ ì—ëŸ¬
    raise ValueError(f"í…œí”Œë¦¿ '{raw}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# â”€â”€ ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° endpoints: /health, /list_templates, /create_xlsx, /daily_news, /render_news", 200

@app.route("/health", methods=["GET"])
def health_check():
    return "OK", 200


# â”€â”€ í…œí”Œë¦¿ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/list_templates", methods=["GET"])
def list_templates():
    path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(path):
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404
    df = pd.read_csv(path, encoding="utf-8-sig")
    templates = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    return jsonify({
        "template_list": templates,
        "alias_keys": sorted(build_alias_map(templates).keys())
    })


# â”€â”€ ì—‘ì…€ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "").strip()
    # â€œì–‘ì‹/ì„œì‹/ì ê²€í‘œ/ê³„íšì„œ/í‘œ + (ì„|ë¥¼)? + (ì£¼ì„¸ìš”|ì¤˜)?â€ ì œê±°
    raw = re.sub(
        r"\s*(?:ì–‘ì‹|ì„œì‹|ì ê²€í‘œ|ê³„íšì„œ|í‘œ)(?:ì„|ë¥¼)?\s*(?:ì£¼ì„¸ìš”|ì¤˜)?$",
        "",
        raw,
        flags=re.IGNORECASE
    ).strip()

    path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(path):
        return jsonify(error="í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 404

    df = pd.read_csv(path, encoding="utf-8-sig")
    if "í…œí”Œë¦¿ëª…" not in df.columns:
        return jsonify(error="í•„ìš”í•œ 'í…œí”Œë¦¿ëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."), 500

    templates = sorted(df["í…œí”Œë¦¿ëª…"].dropna().unique().tolist())
    alias_map = build_alias_map(templates)

    try:
        tpl = resolve_keyword(raw, templates, alias_map)
        out_df = df[df["í…œí”Œë¦¿ëª…"] == tpl][
            ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]
        ]
    except ValueError:
        # fallback: GPTì—ê²Œ JSON ë°°ì—´ ìƒì„± ìš”ì²­
        system = {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ ì‚°ì—…ì•ˆì „ ë¬¸ì„œ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
                "ë‹¤ìŒ ì»¬ëŸ¼(ì‘ì—… í•­ëª©, ì‘ì„± ì–‘ì‹, ì‹¤ë¬´ ì˜ˆì‹œ 1, ì‹¤ë¬´ ì˜ˆì‹œ 2)ì„ ê°€ì§„ JSON ë°°ì—´ì„ 5ê°œ ì´ìƒ ìƒì„±í•´ì£¼ì„¸ìš”.\n"
                f"í…œí”Œë¦¿ëª…: {raw}"
            )
        }
        user = {
            "role": "user",
            "content": f"í…œí”Œë¦¿ëª… '{raw}'ì˜ ê³ ë„í™”ëœ ì–‘ì‹ì„ JSON ë°°ì—´ë¡œ ì£¼ì„¸ìš”."
        }
        resp = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[system, user],
            max_tokens=800,
            temperature=0.7
        )
        content = resp.choices[0].message.content
        try:
            data = json.loads(content)
            out_df = pd.DataFrame(data)
        except:
            out_df = pd.DataFrame([{
                "ì‘ì—… í•­ëª©": raw,
                "ì‘ì„± ì–‘ì‹": content.replace("\n", " "),
                "ì‹¤ë¬´ ì˜ˆì‹œ 1": "",
                "ì‹¤ë¬´ ì˜ˆì‹œ 2": ""
            }])

    # Excel ìƒì„± & í¬ë§·íŒ…
    wb = Workbook()
    ws = wb.active
    headers = ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]
    ws.append(headers)
    for cell in ws[1]:
        cell.font = Font(bold=True)
        cell.alignment = Alignment(horizontal="center")

    for row in out_df.itertuples(index=False):
        ws.append(row)

    # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • & wrap_text
    for idx, col in enumerate(ws.columns, 1):
        max_len = max(len(str(c.value)) for c in col)
        letter = get_column_letter(idx)
        ws.column_dimensions[letter].width = min(max_len + 2, 60)
        if headers[idx-1] == "ì‘ì„± ì–‘ì‹":
            for c in col[1:]:
                c.alignment = Alignment(wrap_text=True)

    buf = BytesIO()
    wb.save(buf)
    buf.seek(0)
    fname = f"{tpl + '.xlsx' if 'tpl' in locals() else raw + '.xlsx'}"
    disp  = "attachment; filename*=UTF-8''" + quote(fname)
    return Response(
        buf.read(),
        headers={
            "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "Content-Disposition": disp,
            "Cache-Control": "public, max-age=3600"
        }
    )


# â”€â”€ ë‰´ìŠ¤ í¬ë¡¤ë§ / ë Œë”ë§ ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_safetynews_article_content(url):
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else ""
    except:
        return ""


def crawl_naver_news():
    base = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    kws = ["ê±´ì„¤ ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in kws:
        r = requests.get(
            base,
            headers=headers,
            params={"query": kw, "display": 2, "sort": "date"},
            timeout=10
        )
        if r.status_code != 200:
            continue
        for item in r.json().get("items", []):
            title = BeautifulSoup(item["title"], "html.parser").get_text()
            desc  = BeautifulSoup(item["description"], "html.parser").get_text()
            out.append({
                "ì¶œì²˜": item.get("originallink", "ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": item.get("link", ""),
                "ë‚ ì§œ": item.get("pubDate", ""),
                "ë³¸ë¬¸": desc
            })
    return out


def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    kws = ["ê±´ì„¤ ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in kws:
        r = requests.get(f"{base}/search/news?searchword={kw}", headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        if r.status_code != 200:
            continue
        soup = BeautifulSoup(r.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else ""
            d = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out


@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200
    return jsonify(news)


@app.route("/render_news", methods=["GET"])
def render_news():
    news = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in news:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)
    items = sorted(filtered, key=lambda x: parser.parse(x["ë‚ ì§œ"]), reverse=True)[:3]
    if not items:
        return jsonify(error="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."), 200

    template = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{title}â€ ({date}, {ì¶œì²˜})\n\n"
        "{ë³¸ë¬¸}\n"
        "ğŸ” ë” ë³´ë ¤ë©´ â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )
    system_msg = {"role": "system", "content": f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template}"}
    user_msg   = {"role": "user",   "content": str(items)}
    resp       = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[system_msg, user_msg],
        max_tokens=800,
        temperature=0.7
    )
    return jsonify(formatted_news=resp.choices[0].message.content)


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))

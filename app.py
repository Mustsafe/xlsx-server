from flask import Flask, request, send_file, jsonify, send_from_directory
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import openai
from dateutil import parser
from datetime import datetime, timedelta
from io import BytesIO

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # í•œê¸€ ê¹¨ì§ ë°©ì§€

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai.api_key = os.getenv("OPENAI_API_KEY")

# ./data ë””ë ‰í† ë¦¬ ì‚¬ìš©
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì„œë¹™
@app.route("/.well-known/<path:filename>")
def serve_well_known(filename):
    return send_from_directory(
        os.path.join(app.root_path, "static", ".well-known"),
        filename,
        mimetype="application/json"
    )

# OpenAPI ë° ë¡œê³  íŒŒì¼ ì„œë¹™
@app.route("/openapi.json")
def serve_openapi():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "openapi.json",
        mimetype="application/json"
    )

@app.route("/logo.png")
def serve_logo():
    return send_from_directory(
        os.path.join(app.root_path, "static"),
        "logo.png",
        mimetype="image/png"
    )

# ë„¤ì´ë²„ ì˜¤í”ˆ API ìê²©ì¦ëª…
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# í‚¤ì›Œë“œ ë§¤í•‘
KEYWORD_ALIAS = {
    "ê³ ì†Œì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œ ì‘ì—… ê³„íšì„œ": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ê³ ì†Œì‘ì—…": "ê³ ì†Œì‘ì—…ëŒ€ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„ ê³„íšì„œ": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "ë°€íê³µê°„": "ë°€íê³µê°„ì‘ì—…ê³„íšì„œ",
    "í•´ì²´ ì‘ì—…ê³„íšì„œ": "í•´ì²´ì‘ì—…ê³„íšì„œ",
    "í¬ë ˆì¸ ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ë¹„ê³„ ì‘ì—… ê³„íšì„œ": "ë¹„ê³„ì‘ì—…ê³„íšì„œ",
    "í˜‘ì°© ì‘ì—… ê³„íšì„œ": "í˜‘ì°©ìœ„í—˜ì‘ì—…ê³„íšì„œ",
    "ì–‘ì¤‘ê¸° ì‘ì—…ê³„íšì„œ": "í¬ë ˆì¸ì‘ì—…ê³„íšì„œ",
    "ê³ ì••ê°€ìŠ¤ ì‘ì—… ê³„íšì„œ": "ê³ ì••ê°€ìŠ¤ì‘ì—…ê³„íšì„œ"
}

def resolve_keyword(raw_keyword: str) -> str:
    for alias, std in KEYWORD_ALIAS.items():
        if alias in raw_keyword:
            return std
    return raw_keyword

@app.route("/", methods=["GET"])
def index():
    return "ğŸ“° ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸: /daily_news, /render_news, /create_xlsx", 200

# â•â•â•â• XLSX ìƒì„± ì—”ë“œí¬ì¸íŠ¸ â•â•â•â•
@app.route("/create_xlsx", methods=["GET"])
def create_xlsx():
    raw = request.args.get("template", "")
    tpl = resolve_keyword(raw)

    csv_path = os.path.join(DATA_DIR, "í†µí•©_ë…¸ì§€íŒŒì¼.csv")
    if not os.path.exists(csv_path):
        return {"error": "í†µí•© CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 404

    df = pd.read_csv(csv_path)

    # 'ì‘ì—… í•­ëª©' ì»¬ëŸ¼ì„ ì‚¬ìš©í•œ í•„í„°ë§ ë¡œì§
    if "ì‘ì—… í•­ëª©" in df.columns:
        mask = df["ì‘ì—… í•­ëª©"].astype(str).str.contains(tpl)
        filtered = df[mask]
    else:
        return {"error": "í•„ìš”í•œ 'ì‘ì—… í•­ëª©' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."}, 500

    if filtered.empty:
        return {"error": f"'{tpl}' ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, 404

    # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
    columns_to_use = ["ì‘ì—… í•­ëª©", "ì‘ì„± ì–‘ì‹", "ì‹¤ë¬´ ì˜ˆì‹œ 1", "ì‹¤ë¬´ ì˜ˆì‹œ 2"]
    out_df = filtered[columns_to_use]

    # ë©”ëª¨ë¦¬ ìƒì—ì„œ ì—‘ì…€ íŒŒì¼ ìƒì„±
    output = BytesIO()
    out_df.to_excel(output, index=False)
    output.seek(0)

    return send_file(
        output,
        as_attachment=True,
        download_name=f"{tpl}.xlsx",
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

# SafetyNews ë³¸ë¬¸ ì¶”ì¶œ
def fetch_safetynews_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        node = soup.select_one("div#article-view-content-div")
        return node.get_text("\n").strip() if node else "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"
    except Exception:
        return "(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨)"

# ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
def crawl_naver_news():
    base_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    keywords = ["ê±´ì„¤ ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        params = {"query": kw, "display": 2, "sort": "date"}
        resp = requests.get(base_url, headers=headers, params=params, timeout=10)
        if resp.status_code != 200:
            continue
        for item in resp.json().get("items", []):
            title = BeautifulSoup(item.get("title", ""), "html.parser").get_text()
            desc = BeautifulSoup(item.get("description", ""), "html.parser").get_text()
            out.append({
                "ì¶œì²˜": item.get("originallink", "ë„¤ì´ë²„"),
                "ì œëª©": title,
                "ë§í¬": item.get("link", ""),
                "ë‚ ì§œ": item.get("pubDate", ""),
                "ë³¸ë¬¸": desc
            })
    return out

# SafetyNews í¬ë¡¤ë§
def crawl_safetynews():
    base = "https://www.safetynews.co.kr"
    keywords = ["ê±´ì„¤ ì‚¬ê³ ", "ì¶”ë½ ì‚¬ê³ ", "ë¼ì„ ì‚¬ê³ ", "ì§ˆì‹ ì‚¬ê³ ", "í­ë°œ ì‚¬ê³ ", "ì‚°ì—…ì¬í•´", "ì‚°ì—…ì•ˆì „"]
    out = []
    for kw in keywords:
        resp = requests.get(f"{base}/search/news?searchword={kw}", headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        if resp.status_code != 200:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select(".article-list-content")[:2]:
            t = item.select_one(".list-titles")
            href = base + t["href"] if t and t.get("href") else None
            d = item.select_one(".list-dated")
            content = fetch_safetynews_article_content(href) if href else ""
            out.append({
                "ì¶œì²˜": "ì•ˆì „ì‹ ë¬¸",
                "ì œëª©": t.get_text(strip=True) if t else "",
                "ë§í¬": href,
                "ë‚ ì§œ": d.get_text(strip=True) if d else "",
                "ë³¸ë¬¸": content[:1000]
            })
    return out

# ì›ë³¸ ë‰´ìŠ¤ JSON ë°˜í™˜
@app.route("/daily_news", methods=["GET"])
def get_daily_news():
    news = crawl_naver_news() + crawl_safetynews()
    if not news:
        return jsonify({"error": "ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200
    return jsonify(news)

# GPT í¬ë§· ë‰´ìŠ¤ ë°˜í™˜
@app.route("/render_news", methods=["GET"])
def render_news():
    raw = crawl_naver_news() + crawl_safetynews()
    cutoff = datetime.utcnow() - timedelta(days=3)
    filtered = []
    for n in raw:
        try:
            dt = parser.parse(n["ë‚ ì§œ"])
        except:
            continue
        if dt >= cutoff:
            n["ë‚ ì§œ"] = dt.strftime("%Y.%m.%d")
            filtered.append(n)
    news_items = sorted(filtered, key=lambda x: parser.parse(x["ë‚ ì§œ"]), reverse=True)[:3]
    if not news_items:
        return jsonify({"error": "ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."}), 200

    template_text = (
        "ğŸ“Œ ì‚°ì—… ì•ˆì „ ë° ë³´ê±´ ìµœì‹  ë‰´ìŠ¤\n"
        "ğŸ“° â€œ{title}â€ ({date}, {source})\n\n"
        "{headline}\n"
        "ğŸ” {recommendation}\n"
        "ğŸ‘‰ ìš”ì•½ ì œê³µë¨ Â· â€œë‰´ìŠ¤ ë” ë³´ì—¬ì¤˜â€ ì…ë ¥ ì‹œ ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€ í™•ì¸ ê°€ëŠ¥"
    )
    system_message = {
        "role": "system",
        "content": f"ë‹¤ìŒ JSON í˜•ì‹ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ ì•„ë˜ í…œí”Œë¦¿ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”.\ní…œí”Œë¦¿:\n{template_text}"
    }
    user_message = {"role": "user", "content": str(news_items)}

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[system_message, user_message],
        max_tokens=800,
        temperature=0.7
    )
    return jsonify({"formatted_news": resp.choices[0].message.content})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))
